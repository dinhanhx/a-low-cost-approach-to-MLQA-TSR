{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29aad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhvd27/miniforge3/envs/mlqa-tsr/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/anhvd27/miniforge3/envs/mlqa-tsr/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anhvd27/projects/vlsp/mlqa-tsr/codebase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhvd27/miniforge3/envs/mlqa-tsr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd \"full/path/to/codebase\"\n",
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "from qdrant_client.models import FieldCondition, Filter, MatchValue, Prefetch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.dataset import LawCorpus, TestDataset\n",
    "from src.db.qdrant import Qdrant\n",
    "from src.nn.text_embedding import TextEmbedding\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7721fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    }
   ],
   "source": [
    "text_embedding_model = TextEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8fbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(\n",
    "    image: Image.Image,\n",
    "    max_height: int = 1536,\n",
    "    max_width: int = 1536,\n",
    "):\n",
    "    if image.width <= max_width and image.height <= max_height:\n",
    "        return image\n",
    "\n",
    "    if image.width >= image.height:\n",
    "        new_width = max_width\n",
    "        new_height = new_width * image.height // image.width\n",
    "        image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        return image\n",
    "\n",
    "    if image.height >= image.width:\n",
    "        new_height = max_height\n",
    "        new_width = new_height * image.width // image.height\n",
    "        image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        return image\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def encode_image(image_path: str):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = resize_image(image)\n",
    "    image_buffer = BytesIO()\n",
    "    image.save(image_buffer, format=\"JPEG\")\n",
    "    return base64.b64encode(image_buffer.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=os.getenv(\"VLLM_API_KEY\"),\n",
    "    base_url=os.getenv(\"VLLM_BASE_URL\"),\n",
    ")\n",
    "model_name = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "\n",
    "\n",
    "class Result(BaseModel):\n",
    "    choice: str\n",
    "    # explanation: str\n",
    "\n",
    "\n",
    "system_prompt = \"\"\" Given an image and a question both about traffic in Vietnam,\n",
    "Multiple choices and yes/no questions shall be provided,\n",
    "If A, B, C, D were given, choose the letter only,\n",
    "If Đúng; Sai were given, choose Đúng or Sai only,\n",
    "No need explanation needed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant()\n",
    "law_corpus = LawCorpus(Path(\"../data/law_db\"))\n",
    "# path to private test folder, perhaps one need to slightly change the file names and image path\n",
    "# read the code in src/data/dataset.py\n",
    "test_set_2 = TestDataset(Path(\"../data/private_test\"), law_corpus, task_number=2)\n",
    "image_feature_df = pl.read_json(\"assets/private_test_image_feature.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7468c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(data_point: dict):\n",
    "    fmt_text_input = \"Question: \" + data_point[\"question\"] + \"\\nOptions:\\n\"\n",
    "    if data_point[\"question_type\"] == \"Multiple choice\":\n",
    "        for k, v in data_point[\"choices\"].items():\n",
    "            fmt_text_input += f\"{k}: {v} \\n\"\n",
    "        default_choice = \"A\"\n",
    "    elif data_point[\"question_type\"] == \"Yes/No\":\n",
    "        fmt_text_input += \"Đúng\\nSai\\n\"\n",
    "        default_choice = \"Đúng\"\n",
    "    fmt_text_input += \"Choice: \"\n",
    "    if data_point.get(\"answer\", \"\"):\n",
    "        fmt_text_input += data_point[\"answer\"]\n",
    "    return fmt_text_input, default_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad408f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [04:43<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"train_data\"\n",
    "for i in tqdm(range(len(test_set_2))):\n",
    "    image_path = test_set_2[i][\"image_path\"]\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    fmt_text_input, default_choice = format_text(test_set_2[i])\n",
    "\n",
    "    image_feature_point = image_feature_df.filter(pl.col(\"image_name\") == Path(image_path).name).row(0, named=True)\n",
    "    image_object_feature_list_vector = [[0.0] * 2304]\n",
    "    if image_feature_point[\"object_feature_list\"]:\n",
    "        image_object_feature_list_vector = [\n",
    "            img_obj[\"object_feature\"] for img_obj in image_feature_point[\"object_feature_list\"]\n",
    "        ]\n",
    "\n",
    "    someshots = qdrant.client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=Prefetch(\n",
    "            prefetch=Prefetch(\n",
    "                query=text_embedding_model.infer_single(fmt_text_input),\n",
    "                using=\"text_vector\",\n",
    "                limit=10,\n",
    "                filter=Filter(\n",
    "                    must=FieldCondition(key=\"question_type\", match=MatchValue(value=test_set_2[i][\"question_type\"]))\n",
    "                ),\n",
    "            ),\n",
    "            query=image_feature_point[\"general_feature\"],\n",
    "            using=\"image_general_feature_vector\",\n",
    "            limit=5,\n",
    "        ),\n",
    "        query=image_object_feature_list_vector,\n",
    "        using=\"image_object_feature_list_vector\",\n",
    "        limit=3,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        }\n",
    "    )\n",
    "    if someshots.points:\n",
    "        for shot in someshots.points:\n",
    "            if not shot.payload:\n",
    "                continue\n",
    "            shot_image_path = shot.payload[\"image_path\"]\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": format_text(shot.payload)[0]},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(shot_image_path)}\"},\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": fmt_text_input},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "    completion = openai_client.chat.completions.parse(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        response_format=Result,\n",
    "    )\n",
    "    model_result = completion.choices[0].message.parsed\n",
    "    if model_result is None:\n",
    "        test_set_2.index_result(index=i, answer=default_choice)\n",
    "    if model_result:\n",
    "        test_set_2.index_result(index=i, answer=model_result.choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb0206f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_2.save_results(\"assets/private_submissions/submission_task2.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlqa-tsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
